{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coder-gx/my_lcm/blob/main/train_lcm_lora_sdxl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLY2x_dF7jrV",
        "outputId": "fb39f904-4c4e-40df-8567-750c932583a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/diffusers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9-bTKnk9-CC",
        "outputId": "77fd704e-3651-42fe-c53b-8a56c8eaa925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 48405, done.\u001b[K\n",
            "remote: Counting objects: 100% (13256/13256), done.\u001b[K\n",
            "remote: Compressing objects: 100% (906/906), done.\u001b[K\n",
            "remote: Total 48405 (delta 12782), reused 12464 (delta 12296), pack-reused 35149\u001b[K\n",
            "Receiving objects: 100% (48405/48405), 31.45 MiB | 22.74 MiB/s, done.\n",
            "Resolving deltas: 100% (36034/36034), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 列出当前目录下的文件和文件夹\n",
        "print(\"当前目录内容:\", os.listdir())\n",
        "\n",
        "# 切换到下一层目录\n",
        "next_directory = \"drive/MyDrive/LCM/diffusers\"\n",
        "os.chdir(next_directory)\n",
        "\n",
        "# 打印切换后的当前目录内容\n",
        "print(\"切换后的目录内容:\", os.listdir())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlC8RsDP-82q",
        "outputId": "c96ebf07-67b8-4c98-bc25-1c9b7466e629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前目录内容: ['.config', 'drive', 'sample_data']\n",
            "切换后的目录内容: ['Makefile', 'docker', '.github', 'docs', 'pyproject.toml', 'MANIFEST.in', 'PHILOSOPHY.md', 'setup.py', 'CITATION.cff', 'README.md', 'LICENSE', 'CODE_OF_CONDUCT.md', 'utils', '_typos.toml', 'benchmarks', '.git', 'tests', 'src', 'examples', 'CONTRIBUTING.md', 'scripts', '.gitignore', 'model', '.ipynb_checkpoints']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjCg4eWAAEVS",
        "outputId": "12628019-520b-4d60-d42a-684c0d364582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/drive/MyDrive/LCM/diffusers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.25.0.dev0) (7.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.25.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.25.0.dev0) (0.19.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.25.0.dev0) (1.23.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.25.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.25.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.25.0.dev0) (0.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.25.0.dev0) (9.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0.dev0) (23.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.25.0.dev0) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.25.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.25.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.25.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.25.0.dev0) (2023.11.17)\n",
            "Building wheels for collected packages: diffusers\n",
            "  Building editable for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.25.0.dev0-0.editable-py3-none-any.whl size=11082 sha256=45dc0c4468af273a84c1af1e47133618e573554364975942815f61e229d73f2e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zgvghn63/wheels/50/04/32/4930e4c504d1ff46b7d8857929339dcc22786047c509554e2b\n",
            "Successfully built diffusers\n",
            "Installing collected packages: diffusers\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.25.0.dev0\n",
            "    Uninstalling diffusers-0.25.0.dev0:\n",
            "      Successfully uninstalled diffusers-0.25.0.dev0\n",
            "Successfully installed diffusers-0.25.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r examples/consistency_distillation/requirements.txt"
      ],
      "metadata": {
        "id": "ZaqOb_FyAjeA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37d4a2c6-17e4-41ff-eb31-6b9da8095d3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from -r examples/consistency_distillation/requirements.txt (line 1)) (0.25.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r examples/consistency_distillation/requirements.txt (line 2)) (0.16.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from -r examples/consistency_distillation/requirements.txt (line 3)) (4.35.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from -r examples/consistency_distillation/requirements.txt (line 4)) (6.1.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r examples/consistency_distillation/requirements.txt (line 5)) (2.15.1)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from -r examples/consistency_distillation/requirements.txt (line 6)) (3.1.2)\n",
            "Requirement already satisfied: webdataset in /usr/local/lib/python3.10/dist-packages (from -r examples/consistency_distillation/requirements.txt (line 7)) (0.2.86)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->-r examples/consistency_distillation/requirements.txt (line 2)) (2.31.0)\n",
            "Collecting torch>=1.10.0 (from accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1))\n",
            "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r examples/consistency_distillation/requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (12.3.101)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r examples/consistency_distillation/requirements.txt (line 3)) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r examples/consistency_distillation/requirements.txt (line 3)) (0.15.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r examples/consistency_distillation/requirements.txt (line 3)) (4.66.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->-r examples/consistency_distillation/requirements.txt (line 4)) (0.2.12)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->-r examples/consistency_distillation/requirements.txt (line 6)) (2.1.3)\n",
            "Requirement already satisfied: braceexpand in /usr/local/lib/python3.10/dist-packages (from webdataset->-r examples/consistency_distillation/requirements.txt (line 7)) (0.1.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r examples/consistency_distillation/requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r examples/consistency_distillation/requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r examples/consistency_distillation/requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r examples/consistency_distillation/requirements.txt (line 2)) (2023.11.17)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r examples/consistency_distillation/requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate>=0.16.0->-r examples/consistency_distillation/requirements.txt (line 1)) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.2\n",
            "    Uninstalling torch-2.1.2:\n",
            "      Successfully uninstalled torch-2.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xformers 0.0.23.post1 requires torch==2.1.2, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate.utils import write_basic_config\n",
        "write_basic_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HjQSNanBAgl",
        "outputId": "25f460ff-b080-4e6d-ec05-564fe7c7efbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5339mtbcSqgA",
        "outputId": "beafa6be-9431-4688-e9ef-d1eb425da46a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.25.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.3.101)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7C4ZN7PgVAh-",
        "outputId": "b5993e1e-19db-463b-83bc-8fd7262968ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xformers\n",
            "  Downloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.23.5)\n",
            "Collecting torch==2.1.2 (from xformers)\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.2->xformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2->xformers) (2.1.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->xformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2->xformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2->xformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.1.2 xformers-0.0.23.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8SPSEGUWf1t",
        "outputId": "05821af2-0231-49ef-f8ff-b9bb6cf68051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.41.3.post2-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.41.3.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
        "#export OUTPUT_DIR=\"path/to/saved/model\"\n",
        "\n",
        "!accelerate launch examples/consistency_distillation/train_lcm_distill_lora_sd_wds.py \\\n",
        "    --pretrained_teacher_model=runwayml/stable-diffusion-v1-5 \\\n",
        "    --output_dir=model100 \\\n",
        "    --mixed_precision=fp16 \\\n",
        "    --resolution=512 \\\n",
        "    --lora_rank=64 \\\n",
        "    --learning_rate=1e-6 --loss_type=\"huber\" --adam_weight_decay=0.0 \\\n",
        "    --max_train_steps=100 \\\n",
        "    --max_train_samples=400000 \\\n",
        "    --dataloader_num_workers=8 \\\n",
        "    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true\" \\\n",
        "    --validation_steps=100 \\\n",
        "    --checkpointing_steps=20 --checkpoints_total_limit=4 \\\n",
        "    --train_batch_size=2 \\\n",
        "    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\n",
        "    --gradient_accumulation_steps=1 \\\n",
        "    --use_8bit_adam \\\n",
        "    --resume_from_checkpoint=latest \\\n",
        "    --report_to=wandb \\\n",
        "    --seed=453645634"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV_m65_8JxbJ",
        "outputId": "d28c981f-877f-4d5b-adac-56f637dac511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-18 09:28:29.271278: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-18 09:28:29.271332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-18 09:28:29.272581: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-18 09:28:30.459403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:384: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
            "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
            "12/18/2023 09:28:31 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "{'timestep_spacing', 'thresholding', 'clip_sample_range', 'dynamic_thresholding_ratio', 'prediction_type', 'variance_type', 'sample_max_value'} was not found in config. Values will be initialized to default values.\n",
            "{'force_upcast', 'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "{'encoder_hid_dim_type', 'conv_out_kernel', 'time_embedding_dim', 'upcast_attention', 'cross_attention_norm', 'time_embedding_act_fn', 'addition_embed_type', 'mid_block_only_cross_attention', 'resnet_skip_time_act', 'timestep_post_act', 'projection_class_embeddings_input_dim', 'only_cross_attention', 'class_embed_type', 'mid_block_type', 'num_attention_heads', 'time_cond_proj_dim', 'conv_in_kernel', 'num_class_embeds', 'transformer_layers_per_block', 'addition_embed_type_num_heads', 'attention_type', 'reverse_transformer_layers_per_block', 'class_embeddings_concat', 'addition_time_embed_dim', 'use_linear_projection', 'dual_cross_attention', 'time_embedding_type', 'resnet_out_scale_factor', 'resnet_time_scale_shift', 'encoder_hid_dim', 'dropout'} was not found in config. Values will be initialized to default values.\n",
            "{'encoder_hid_dim_type', 'conv_out_kernel', 'time_embedding_dim', 'upcast_attention', 'cross_attention_norm', 'time_embedding_act_fn', 'addition_embed_type', 'mid_block_only_cross_attention', 'resnet_skip_time_act', 'timestep_post_act', 'projection_class_embeddings_input_dim', 'only_cross_attention', 'class_embed_type', 'mid_block_type', 'num_attention_heads', 'time_cond_proj_dim', 'conv_in_kernel', 'num_class_embeds', 'transformer_layers_per_block', 'addition_embed_type_num_heads', 'attention_type', 'reverse_transformer_layers_per_block', 'class_embeddings_concat', 'addition_time_embed_dim', 'use_linear_projection', 'dual_cross_attention', 'time_embedding_type', 'resnet_out_scale_factor', 'resnet_time_scale_shift', 'encoder_hid_dim', 'dropout'} was not found in config. Values will be initialized to default values.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "12/18/2023 09:28:41 - INFO - __main__ - ***** Running training *****\n",
            "12/18/2023 09:28:41 - INFO - __main__ -   Num batches each epoch = 200000\n",
            "12/18/2023 09:28:41 - INFO - __main__ -   Num Epochs = 1\n",
            "12/18/2023 09:28:41 - INFO - __main__ -   Instantaneous batch size per device = 2\n",
            "12/18/2023 09:28:41 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "12/18/2023 09:28:41 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "12/18/2023 09:28:41 - INFO - __main__ -   Total optimization steps = 100\n",
            "Checkpoint 'latest' does not exist. Starting a new training run.\n",
            "Steps:  20% 20/100 [00:51<02:54,  2.19s/it, loss=0.0148, lr=1e-6]12/18/2023 09:29:33 - INFO - accelerate.accelerator - Saving current state to model100/checkpoint-20\n",
            "Model weights saved in model100/checkpoint-20/unet_lora/pytorch_lora_weights.safetensors\n",
            "12/18/2023 09:29:39 - INFO - accelerate.checkpointing - Optimizer state saved in model100/checkpoint-20/optimizer.bin\n",
            "12/18/2023 09:29:39 - INFO - accelerate.checkpointing - Scheduler state saved in model100/checkpoint-20/scheduler.bin\n",
            "12/18/2023 09:29:39 - INFO - accelerate.checkpointing - Gradient scaler state saved in model100/checkpoint-20/scaler.pt\n",
            "12/18/2023 09:29:39 - INFO - accelerate.checkpointing - Random states saved in model100/checkpoint-20/random_states_0.pkl\n",
            "12/18/2023 09:29:39 - INFO - __main__ - Saved state to model100/checkpoint-20\n",
            "Steps:  40% 40/100 [01:43<02:14,  2.25s/it, loss=0.0266, lr=1e-6]12/18/2023 09:30:24 - INFO - accelerate.accelerator - Saving current state to model100/checkpoint-40\n",
            "Model weights saved in model100/checkpoint-40/unet_lora/pytorch_lora_weights.safetensors\n",
            "12/18/2023 09:30:31 - INFO - accelerate.checkpointing - Optimizer state saved in model100/checkpoint-40/optimizer.bin\n",
            "12/18/2023 09:30:31 - INFO - accelerate.checkpointing - Scheduler state saved in model100/checkpoint-40/scheduler.bin\n",
            "12/18/2023 09:30:31 - INFO - accelerate.checkpointing - Gradient scaler state saved in model100/checkpoint-40/scaler.pt\n",
            "12/18/2023 09:30:31 - INFO - accelerate.checkpointing - Random states saved in model100/checkpoint-40/random_states_0.pkl\n",
            "12/18/2023 09:30:31 - INFO - __main__ - Saved state to model100/checkpoint-40\n",
            "Steps:  60% 60/100 [02:35<01:31,  2.30s/it, loss=0.0505, lr=1e-6]12/18/2023 09:31:17 - INFO - accelerate.accelerator - Saving current state to model100/checkpoint-60\n",
            "Model weights saved in model100/checkpoint-60/unet_lora/pytorch_lora_weights.safetensors\n",
            "12/18/2023 09:31:22 - INFO - accelerate.checkpointing - Optimizer state saved in model100/checkpoint-60/optimizer.bin\n",
            "12/18/2023 09:31:23 - INFO - accelerate.checkpointing - Scheduler state saved in model100/checkpoint-60/scheduler.bin\n",
            "12/18/2023 09:31:23 - INFO - accelerate.checkpointing - Gradient scaler state saved in model100/checkpoint-60/scaler.pt\n",
            "12/18/2023 09:31:23 - INFO - accelerate.checkpointing - Random states saved in model100/checkpoint-60/random_states_0.pkl\n",
            "12/18/2023 09:31:23 - INFO - __main__ - Saved state to model100/checkpoint-60\n",
            "Steps:  80% 80/100 [03:28<00:46,  2.33s/it, loss=0.0229, lr=1e-6]12/18/2023 09:32:10 - INFO - accelerate.accelerator - Saving current state to model100/checkpoint-80\n",
            "Model weights saved in model100/checkpoint-80/unet_lora/pytorch_lora_weights.safetensors\n",
            "12/18/2023 09:32:14 - INFO - accelerate.checkpointing - Optimizer state saved in model100/checkpoint-80/optimizer.bin\n",
            "12/18/2023 09:32:14 - INFO - accelerate.checkpointing - Scheduler state saved in model100/checkpoint-80/scheduler.bin\n",
            "12/18/2023 09:32:14 - INFO - accelerate.checkpointing - Gradient scaler state saved in model100/checkpoint-80/scaler.pt\n",
            "12/18/2023 09:32:15 - INFO - accelerate.checkpointing - Random states saved in model100/checkpoint-80/random_states_0.pkl\n",
            "12/18/2023 09:32:15 - INFO - __main__ - Saved state to model100/checkpoint-80\n",
            "Steps: 100% 100/100 [04:21<00:00,  2.32s/it, loss=0.0178, lr=1e-6]12/18/2023 09:33:02 - INFO - __main__ - 4 checkpoints already exist, removing 1 checkpoints\n",
            "12/18/2023 09:33:02 - INFO - __main__ - removing checkpoints: checkpoint-20\n",
            "12/18/2023 09:33:02 - INFO - accelerate.accelerator - Saving current state to model100/checkpoint-100\n",
            "Model weights saved in model100/checkpoint-100/unet_lora/pytorch_lora_weights.safetensors\n",
            "12/18/2023 09:33:08 - INFO - accelerate.checkpointing - Optimizer state saved in model100/checkpoint-100/optimizer.bin\n",
            "12/18/2023 09:33:08 - INFO - accelerate.checkpointing - Scheduler state saved in model100/checkpoint-100/scheduler.bin\n",
            "12/18/2023 09:33:08 - INFO - accelerate.checkpointing - Gradient scaler state saved in model100/checkpoint-100/scaler.pt\n",
            "12/18/2023 09:33:08 - INFO - accelerate.checkpointing - Random states saved in model100/checkpoint-100/random_states_0.pkl\n",
            "12/18/2023 09:33:08 - INFO - __main__ - Saved state to model100/checkpoint-100\n",
            "12/18/2023 09:33:08 - INFO - __main__ - Running validation... \n",
            "{'timestep_spacing', 'thresholding', 'clip_sample_range', 'dynamic_thresholding_ratio', 'prediction_type', 'original_inference_steps', 'rescale_betas_zero_snr', 'sample_max_value', 'timestep_scaling'} was not found in config. Values will be initialized to default values.\n",
            "{'requires_safety_checker', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/6 [00:00<?, ?it/s]\u001b[A{'encoder_hid_dim_type', 'conv_out_kernel', 'time_embedding_dim', 'upcast_attention', 'cross_attention_norm', 'time_embedding_act_fn', 'addition_embed_type', 'mid_block_only_cross_attention', 'resnet_skip_time_act', 'timestep_post_act', 'projection_class_embeddings_input_dim', 'only_cross_attention', 'class_embed_type', 'mid_block_type', 'num_attention_heads', 'time_cond_proj_dim', 'conv_in_kernel', 'num_class_embeds', 'transformer_layers_per_block', 'addition_embed_type_num_heads', 'attention_type', 'reverse_transformer_layers_per_block', 'class_embeddings_concat', 'addition_time_embed_dim', 'use_linear_projection', 'dual_cross_attention', 'time_embedding_type', 'resnet_out_scale_factor', 'resnet_time_scale_shift', 'encoder_hid_dim', 'dropout'} was not found in config. Values will be initialized to default values.\n",
            "Loaded unet as UNet2DConditionModel from `unet` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  33% 2/6 [00:22<00:45, 11.47s/it]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  50% 3/6 [00:23<00:20,  6.74s/it]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "\n",
            "Loading pipeline components...:  83% 5/6 [00:26<00:04,  4.03s/it]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
            "Loading pipeline components...: 100% 6/6 [00:26<00:00,  4.43s/it]\n",
            "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n",
            "Kohya-style checkpoint detected.\n",
            "Loading unet.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/LCM/diffusers/examples/consistency_distillation/train_lcm_distill_lora_sd_wds.py\", line 1374, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/LCM/diffusers/examples/consistency_distillation/train_lcm_distill_lora_sd_wds.py\", line 1352, in main\n",
            "    log_validation(vae, unet, args, accelerator, weight_dtype, global_step)\n",
            "  File \"/content/drive/MyDrive/LCM/diffusers/examples/consistency_distillation/train_lcm_distill_lora_sd_wds.py\", line 274, in log_validation\n",
            "    images = pipeline(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/LCM/diffusers/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py\", line 994, in __call__\n",
            "    noise_pred = self.unet(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/LCM/diffusers/src/diffusers/models/unet_2d_condition.py\", line 1112, in forward\n",
            "    sample, res_samples = downsample_block(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/LCM/diffusers/src/diffusers/models/unet_2d_blocks.py\", line 1160, in forward\n",
            "    hidden_states = attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/LCM/diffusers/src/diffusers/models/transformer_2d.py\", line 392, in forward\n",
            "    hidden_states = block(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/LCM/diffusers/src/diffusers/models/attention.py\", line 323, in forward\n",
            "    attn_output = self.attn2(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/LCM/diffusers/src/diffusers/models/attention_processor.py\", line 527, in forward\n",
            "    return self.processor(\n",
            "  File \"/content/drive/MyDrive/LCM/diffusers/src/diffusers/models/attention_processor.py\", line 1175, in __call__\n",
            "    hidden_states = xformers.ops.memory_efficient_attention(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py\", line 223, in memory_efficient_attention\n",
            "    return _memory_efficient_attention(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py\", line 321, in _memory_efficient_attention\n",
            "    return _memory_efficient_attention_forward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/__init__.py\", line 334, in _memory_efficient_attention_forward\n",
            "    inp.validate_inputs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/common.py\", line 121, in validate_inputs\n",
            "    raise ValueError(\n",
            "ValueError: Query/Key/Value should either all have the same dtype, or (in the quantized case) Key/Value should have dtype torch.int32\n",
            "  query.dtype: torch.float32\n",
            "  key.dtype  : torch.float16\n",
            "  value.dtype: torch.float16\n",
            "Steps: 100% 100/100 [05:57<00:00,  3.58s/it, loss=0.0178, lr=1e-6]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1017, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 637, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', 'examples/consistency_distillation/train_lcm_distill_lora_sd_wds.py', '--pretrained_teacher_model=runwayml/stable-diffusion-v1-5', '--output_dir=model100', '--mixed_precision=fp16', '--resolution=512', '--lora_rank=64', '--learning_rate=1e-6', '--loss_type=huber', '--adam_weight_decay=0.0', '--max_train_steps=100', '--max_train_samples=400000', '--dataloader_num_workers=8', '--train_shards_path_or_url=pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..01099}.tar?download=true', '--validation_steps=100', '--checkpointing_steps=20', '--checkpoints_total_limit=4', '--train_batch_size=2', '--gradient_checkpointing', '--enable_xformers_memory_efficient_attention', '--gradient_accumulation_steps=1', '--use_8bit_adam', '--resume_from_checkpoint=latest', '--report_to=wandb', '--seed=453645634']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
        "#export OUTPUT_DIR=\"path/to/saved/model\"\n",
        "\n",
        "!accelerate launch examples/consistency_distillation/train_lcm_distill_sd_wds.py \\\n",
        "    --pretrained_teacher_model=runwayml/stable-diffusion-v1-5 \\\n",
        "    --output_dir=model \\\n",
        "    --mixed_precision=fp16 \\\n",
        "    --resolution=512 \\\n",
        "    --learning_rate=1e-6 --loss_type=\"huber\" --ema_decay=0.95 --adam_weight_decay=0.0 \\\n",
        "    --max_train_steps=100 \\\n",
        "    --max_train_samples=400000 \\\n",
        "    --dataloader_num_workers=8 \\\n",
        "    --train_shards_path_or_url=\"pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..00001}.tar?download=true\" \\\n",
        "    --validation_steps=20 \\\n",
        "    --checkpointing_steps=20 --checkpoints_total_limit=4 \\\n",
        "    --train_batch_size=1 \\\n",
        "    --gradient_checkpointing --enable_xformers_memory_efficient_attention \\\n",
        "    --gradient_accumulation_steps=1 \\\n",
        "    --use_8bit_adam \\\n",
        "    --resume_from_checkpoint=latest \\\n",
        "    --report_to=wandb \\\n",
        "    --seed=453645634"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAGcRDJOBc9J",
        "outputId": "d91490b2-4dc7-44c0-c359-5da3dd01d58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-18 09:06:22.739573: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-18 09:06:22.739656: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-18 09:06:22.741540: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-18 09:06:24.493976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:384: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
            "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
            "12/18/2023 09:06:26 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "{'thresholding', 'timestep_spacing', 'dynamic_thresholding_ratio', 'sample_max_value', 'variance_type', 'prediction_type', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
            "{'force_upcast', 'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
            "{'upcast_attention', 'reverse_transformer_layers_per_block', 'class_embeddings_concat', 'time_embedding_dim', 'timestep_post_act', 'addition_embed_type', 'addition_embed_type_num_heads', 'time_embedding_type', 'encoder_hid_dim', 'class_embed_type', 'addition_time_embed_dim', 'dual_cross_attention', 'time_cond_proj_dim', 'transformer_layers_per_block', 'dropout', 'conv_in_kernel', 'mid_block_only_cross_attention', 'resnet_time_scale_shift', 'mid_block_type', 'num_class_embeds', 'cross_attention_norm', 'use_linear_projection', 'encoder_hid_dim_type', 'num_attention_heads', 'only_cross_attention', 'attention_type', 'resnet_out_scale_factor', 'conv_out_kernel', 'resnet_skip_time_act', 'projection_class_embeddings_input_dim', 'time_embedding_act_fn'} was not found in config. Values will be initialized to default values.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "12/18/2023 09:07:10 - INFO - __main__ - ***** Running training *****\n",
            "12/18/2023 09:07:10 - INFO - __main__ -   Num batches each epoch = 400000\n",
            "12/18/2023 09:07:10 - INFO - __main__ -   Num Epochs = 1\n",
            "12/18/2023 09:07:10 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
            "12/18/2023 09:07:10 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "12/18/2023 09:07:10 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "12/18/2023 09:07:10 - INFO - __main__ -   Total optimization steps = 100\n",
            "Checkpoint 'latest' does not exist. Starting a new training run.\n",
            "Steps:   0% 0/100 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/LCM/diffusers/examples/consistency_distillation/train_lcm_distill_sd_wds.py\", line 1361, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/LCM/diffusers/examples/consistency_distillation/train_lcm_distill_sd_wds.py\", line 1299, in main\n",
            "    optimizer.step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/optimizer.py\", line 132, in step\n",
            "    self.scaler.step(self.optimizer, closure)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\", line 416, in step\n",
            "    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\", line 315, in _maybe_opt_step\n",
            "    retval = optimizer.step(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/optimizer.py\", line 185, in patched_step\n",
            "    return method(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\", line 68, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\", line 373, in wrapper\n",
            "    out = func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/optim/optimizer.py\", line 266, in step\n",
            "    self.init_state(group, p, gindex, pindex)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/optim/optimizer.py\", line 414, in init_state\n",
            "    state[\"state1\"] = self.get_state_buffer(p, dtype=torch.uint8)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/optim/optimizer.py\", line 306, in get_state_buffer\n",
            "    return torch.zeros_like(p, dtype=dtype, device=p.device)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 21.06 MiB is free. Process 166616 has 14.72 GiB memory in use. Of the allocated memory 14.39 GiB is allocated by PyTorch, and 177.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Steps:   0% 0/100 [00:49<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 47, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1017, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 637, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', 'examples/consistency_distillation/train_lcm_distill_sd_wds.py', '--pretrained_teacher_model=runwayml/stable-diffusion-v1-5', '--output_dir=model', '--mixed_precision=fp16', '--resolution=512', '--learning_rate=1e-6', '--loss_type=huber', '--ema_decay=0.95', '--adam_weight_decay=0.0', '--max_train_steps=100', '--max_train_samples=400000', '--dataloader_num_workers=8', '--train_shards_path_or_url=pipe:curl -L -s https://huggingface.co/datasets/laion/conceptual-captions-12m-webdataset/resolve/main/data/{00000..00001}.tar?download=true', '--validation_steps=20', '--checkpointing_steps=20', '--checkpoints_total_limit=4', '--train_batch_size=1', '--gradient_checkpointing', '--enable_xformers_memory_efficient_attention', '--gradient_accumulation_steps=1', '--use_8bit_adam', '--resume_from_checkpoint=latest', '--report_to=wandb', '--seed=453645634']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/drive/MyDrive/LCM/diffusers/model/checkpoint-40"
      ],
      "metadata": {
        "id": "y04O2-DxJKdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "s_path = '/content/diffusers'\n",
        "d_path = '/content/drive/MyDrive/LCM/'\n",
        "\n",
        "shutil.move(s_path, d_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1iW6LcEXDUSx",
        "outputId": "a47d7cea-9769-41ec-be71-515fdaed13f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/LCM/diffusers'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}